{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StrikeWade\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#Clear TF memory\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load path to custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trainlabel', 'trainimg', 'testimg', 'testlabel']\n"
     ]
    }
   ],
   "source": [
    "load_path = 'data/data4vgg.npz'\n",
    "l = np.load(load_path)\n",
    "\n",
    "print(l.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 train images loaded\n",
      "18 test images loaded\n",
      "37632 dimensional input\n",
      "2 classes\n",
      "shape of 'trainimg' is  (69, 37632)\n",
      "shape of 'testimg' is  (18, 37632)\n"
     ]
    }
   ],
   "source": [
    "train_image = l['trainimg']\n",
    "train_label = l['trainlabel']\n",
    "test_image = l['testimg']\n",
    "test_label = l['testlabel']\n",
    "n_train = train_image.shape[0]\n",
    "n_class = train_label.shape[1]\n",
    "dimension = train_image.shape[1]\n",
    "n_test = test_image.shape[0]\n",
    "\n",
    "print (\"%d train images loaded\" % (n_train))\n",
    "print (\"%d test images loaded\"  % (n_test))\n",
    "print (\"%d dimensional input\"   % (dimension))\n",
    "print (\"%d classes\"             % (n_class))\n",
    "print (\"shape of 'trainimg' is \", train_image.shape)\n",
    "print (\"shape of 'testimg' is \", test_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tensors for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_image_tensor is (69, 112, 112, 3)\n",
      "Shape of test_image_tensor is (18, 112, 112, 3)\n"
     ]
    }
   ],
   "source": [
    "train_image_tensor = np.ndarray((n_train, 112, 112, 3)) # 112 * 112 * 3 = 37632\n",
    "for i in range(n_train):\n",
    "    current_image = train_image[i, :]\n",
    "#     print(current_image.shape)                          # Shape = (37632,)\n",
    "    current_image = np.reshape(a=current_image, newshape=[112, 112, 3])\n",
    "#     print(current_image.shape)                          # Shape_r = (112, 112, 3)\n",
    "    train_image_tensor[i, :, :, :] = current_image\n",
    "print('Shape of train_image_tensor is', train_image_tensor.shape)\n",
    "\n",
    "\n",
    "test_image_tensor = np.ndarray((n_test, 112, 112, 3)) # 112 * 112 * 3 = 37632\n",
    "for i in range(n_test):\n",
    "    current_image = test_image[i, :]\n",
    "#     print(current_image.shape)                          # Shape = (37632,)\n",
    "    current_image = np.reshape(a=current_image, newshape=[112, 112, 3])\n",
    "#     print(current_image.shape)                          # Shape_r = (112, 112, 3)\n",
    "    test_image_tensor[i, :, :, :] = current_image\n",
    "print('Shape of test_image_tensor is', test_image_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions to use pretrained VGG network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(x, weights, bias):\n",
    "    conv = tf.nn.conv2d(x, filter=tf.constant(weights), strides=[1,1,1,1], padding='SAME')\n",
    "    return tf.nn.bias_add(conv, bias)\n",
    "    \n",
    "def pool_layer(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "def preprocess(image, mean_pixel):\n",
    "    return image - mean_pixel\n",
    "    \n",
    "def unprocess(image, mean_pixel):\n",
    "    return image + mean_pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_net(data_path, input_image):\n",
    "    layers = (\n",
    "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
    "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
    "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
    "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
    "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
    "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
    "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
    "        'relu5_3', 'conv5_4', 'relu5_4'\n",
    "    )\n",
    "    \n",
    "    data = scipy.io.loadmat(data_path)\n",
    "    mean = data['normalization'][0][0][0]\n",
    "    mean_pixel = np.mean(a=mean, axis=(0, 1))\n",
    "    weights = data['layers'][0]\n",
    "    net = {}\n",
    "    current_image = input_image\n",
    "    \n",
    "    for i, name in enumerate(layers):\n",
    "        kind = name[:4]\n",
    "        \n",
    "        if kind == 'conv':\n",
    "            kernels, bias = weights[i][0][0][0][0]\n",
    "            #matconvnet: weights are [width, height, in_channels, out_channels]\n",
    "            #tensorflow: weights are [height, width, in_channels, out_channels]\n",
    "            \n",
    "            kernels = np.transpose(a=kernels, axes=(1, 0, 2, 3))\n",
    "            bias = bias.reshape(-1)\n",
    "            current_image = conv_layer(current_image, kernels, bias)\n",
    "            \n",
    "        elif kind == 'relu':\n",
    "            current_image = tf.nn.relu(current_image)\n",
    "            \n",
    "        elif kind == 'pool':\n",
    "            current_image = pool_layer(current_image)\n",
    "        \n",
    "        net[name] = current_image\n",
    "        \n",
    "    assert len(net) == len(layers)\n",
    "    return net, mean_pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from the VGG network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TYPE OF 'train_features' is:  <class 'numpy.ndarray'>\n",
      "SHAPE OF 'train_features' is:  (69, 7, 7, 512)\n",
      "TYPE OF 'test_features' is:  <class 'numpy.ndarray'>\n",
      "SHAPE OF 'test_features' is:  (18, 7, 7, 512)\n",
      "PREPROCESSING DONE\n"
     ]
    }
   ],
   "source": [
    "VGG_PATH = 'pre-trained-model/imagenet-vgg-verydeep-19.mat'\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    image_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 112, 112, 3])\n",
    "    \n",
    "    net_val, mean_pixel = vgg_net(data_path=VGG_PATH, input_image=image_placeholder)\n",
    "    \n",
    "    train_features = net_val['relu5_4'].eval(feed_dict={image_placeholder: train_image_tensor})\n",
    "    test_features = net_val['relu5_4'].eval(feed_dict={image_placeholder: test_image_tensor})\n",
    "    \n",
    "print(\"TYPE OF 'train_features' is: \", type(train_features))\n",
    "print(\"SHAPE OF 'train_features' is: \", train_features.shape)\n",
    "print(\"TYPE OF 'test_features' is: \" , type(test_features))\n",
    "print(\"SHAPE OF 'test_features' is: \", test_features.shape)\n",
    "print(\"PREPROCESSING DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize CNN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE OF 'test_features' is:  (69, 25088)\n",
      "SHAPE OF 'test_features' is:  (18, 25088)\n"
     ]
    }
   ],
   "source": [
    "train_vectorized = np.ndarray((n_train, 7*7*512))\n",
    "test_vectorized = np.ndarray((n_test, 7*7*512))\n",
    "\n",
    "for i in range(n_train):\n",
    "    current_feature = train_features[i, :, :, :]\n",
    "    current_feature_vec = np.reshape(a= current_feature, newshape=[1, -1])\n",
    "    train_vectorized[i, :] = current_feature_vec\n",
    "\n",
    "for i in range(n_test):\n",
    "    current_feature = test_features[i, :, :, :]\n",
    "    current_feature_vec = np.reshape(a= current_feature, newshape=[1, -1])\n",
    "    test_vectorized[i, :] = current_feature_vec\n",
    "    \n",
    "print(\"SHAPE OF 'test_features' is: \", train_vectorized.shape)\n",
    "print(\"SHAPE OF 'test_features' is: \", test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define networks and functions(add 2 layers MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Params\n",
    "learning_rate = 0.0001\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "display_step = 10\n",
    "\n",
    "#Network params\n",
    "n_input = dimension\n",
    "n_classes = n_class\n",
    "\n",
    "#Weights and biases\n",
    "weights = {\n",
    "    'wd1': tf.Variable(tf.random_normal(shape=[7*7*512, 1024], stddev= 0.1)),\n",
    "    'wd2': tf.Variable(tf.random_normal(shape=[1024, n_classes], stddev= 0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bd1': tf.Variable(tf.random_normal(shape=[1024], stddev= 0.1)),\n",
    "    'bd2': tf.Variable(tf.random_normal(shape=[n_classes], stddev= 0.1))\n",
    "}\n",
    "\n",
    "#TF graph placeholder\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 7*7*512])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, n_classes])\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_basic(x, weights, biases, keep_prob):\n",
    "    #Reshape input\n",
    "    input_r = x\n",
    "    \n",
    "    #Vectorize\n",
    "    dense = tf.reshape(input_r, shape=[-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    \n",
    "    #Fc1\n",
    "    fc1 = tf.nn.relu(tf.add(tf.matmul(dense, weights['wd1']), biases['bd1']))\n",
    "    fc1_dr = tf.nn.dropout(fc1, keep_prob=keep_prob)\n",
    "    \n",
    "    #Fc2\n",
    "    out = tf.add(tf.matmul(fc1_dr, weights['wd2']), biases['bd2'])\n",
    "    \n",
    "    #Return everything\n",
    "    out = {\n",
    "        'input_r': input_r,\n",
    "        'dense': dense,\n",
    "        'fc1': fc1,\n",
    "        'fc1_dr': fc1_dr,\n",
    "        'out': out\n",
    "    }\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-235148e4e89c>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = conv_basic(x, weights, biases, keep_prob)['out']\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/200, Loss = 1.732056\n",
      "-->    Train Accuracy:  0.71\n",
      "-->    Test Accuracy:  0.6111111\n",
      "Epoch 010/200, Loss = 0.102751\n",
      "-->    Train Accuracy:  0.95\n",
      "-->    Test Accuracy:  0.7222222\n",
      "Epoch 020/200, Loss = 0.212020\n",
      "-->    Train Accuracy:  0.96\n",
      "-->    Test Accuracy:  0.7222222\n",
      "Epoch 030/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 040/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8333333\n",
      "Epoch 050/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8333333\n",
      "Epoch 060/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 070/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 080/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 090/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 100/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 110/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 120/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 130/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8333333\n",
      "Epoch 140/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 150/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 160/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8333333\n",
      "Epoch 170/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 180/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n",
      "Epoch 190/200, Loss = 0.000000\n",
      "-->    Train Accuracy:  1.0\n",
      "-->    Test Accuracy:  0.8888889\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#training\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    total_batch = int(n_train/batch_size)+1\n",
    "    \n",
    "    #Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        rand_idx = np.random.randint(n_train, size=batch_size)\n",
    "        batch_x = train_vectorized[rand_idx, :]\n",
    "        batch_y = train_label[rand_idx, :]\n",
    "        \n",
    "        #Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.7})\n",
    "        \n",
    "        #Compute avg loss\n",
    "        avg_loss += sess.run(loss, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0}) / total_batch\n",
    "        \n",
    "    #Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print('Epoch %03d/%03d, Loss = %04f' % (epoch, epochs, avg_loss))\n",
    "        \n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x: test_vectorized, y: test_label, keep_prob: 1.0})\n",
    "        \n",
    "        print('-->    Train Accuracy: ', train_accuracy)\n",
    "        print('-->    Test Accuracy: ', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
